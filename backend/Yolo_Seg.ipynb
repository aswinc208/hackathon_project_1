{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.6 ðŸš€ Python-3.11.5 torch-2.1.2+cpu CPU (12th Gen Intel Core(TM) i5-12500H)\n",
      "YOLOv8s-seg summary (fused): 195 layers, 11810560 parameters, 0 gradients, 42.6 GFLOPs\n",
      "\n",
      "Found https://media.roboflow.com/notebooks/examples/dog.jpeg locally at dog.jpeg\n",
      "image 1/1 c:\\Users\\Rahul\\Desktop\\New folder\\dog.jpeg: 640x384 1 person, 1 car, 1 dog, 1 backpack, 1 handbag, 190.7ms\n",
      "Speed: 9.8ms preprocess, 190.7ms inference, 4.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Results saved to \u001b[1mruns\\segment\\predict7\u001b[0m\n",
      "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo task=segment mode=predict model=yolov8s-seg.pt conf=0.25 source='https://media.roboflow.com/notebooks/examples/dog.jpeg' save=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 C:\\Users\\Rahul\\Desktop\\pexels-jopwell-2422290.jpg: 480x640 5 persons, 2 chairs, 64.7ms\n",
      "Speed: 4.6ms preprocess, 64.7ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 0.,  0.,  0.,  0.,  0., 56., 56.])\n",
      "conf: tensor([0.9382, 0.9316, 0.9016, 0.8606, 0.8443, 0.3624, 0.2646])\n",
      "data: tensor([[2.5692e+03, 6.1450e+02, 3.3431e+03, 1.8818e+03, 9.3816e-01, 0.0000e+00],\n",
      "        [1.8646e+03, 8.3511e+02, 2.5331e+03, 2.1338e+03, 9.3156e-01, 0.0000e+00],\n",
      "        [1.5432e+03, 3.9535e+02, 2.0561e+03, 1.9467e+03, 9.0164e-01, 0.0000e+00],\n",
      "        [3.3721e+02, 1.2317e+02, 1.1739e+03, 2.2066e+03, 8.6062e-01, 0.0000e+00],\n",
      "        [1.1653e+03, 7.6282e+02, 1.7101e+03, 2.0830e+03, 8.4432e-01, 0.0000e+00],\n",
      "        [9.6678e+02, 1.0821e+03, 1.6626e+03, 2.0896e+03, 3.6243e-01, 5.6000e+01],\n",
      "        [1.0035e+03, 1.4638e+03, 1.3508e+03, 2.0745e+03, 2.6461e-01, 5.6000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (2591, 3600)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[2956.1382, 1248.1453,  773.8887, 1267.2889],\n",
      "        [2198.8572, 1484.4658,  668.5414, 1298.7124],\n",
      "        [1799.6504, 1171.0433,  512.8403, 1551.3828],\n",
      "        [ 755.5787, 1164.8755,  836.7355, 2083.4199],\n",
      "        [1437.6892, 1422.8983,  544.7275, 1320.1584],\n",
      "        [1314.6909, 1585.8834,  695.8282, 1007.4973],\n",
      "        [1177.1213, 1769.1218,  347.2915,  610.6758]])\n",
      "xywhn: tensor([[0.8211, 0.4817, 0.2150, 0.4891],\n",
      "        [0.6108, 0.5729, 0.1857, 0.5012],\n",
      "        [0.4999, 0.4520, 0.1425, 0.5988],\n",
      "        [0.2099, 0.4496, 0.2324, 0.8041],\n",
      "        [0.3994, 0.5492, 0.1513, 0.5095],\n",
      "        [0.3652, 0.6121, 0.1933, 0.3888],\n",
      "        [0.3270, 0.6828, 0.0965, 0.2357]])\n",
      "xyxy: tensor([[2569.1938,  614.5009, 3343.0825, 1881.7898],\n",
      "        [1864.5865,  835.1097, 2533.1279, 2133.8220],\n",
      "        [1543.2302,  395.3519, 2056.0706, 1946.7347],\n",
      "        [ 337.2110,  123.1655, 1173.9465, 2206.5854],\n",
      "        [1165.3254,  762.8191, 1710.0530, 2082.9775],\n",
      "        [ 966.7768, 1082.1348, 1662.6050, 2089.6321],\n",
      "        [1003.4756, 1463.7839, 1350.7671, 2074.4597]])\n",
      "xyxyn: tensor([[0.7137, 0.2372, 0.9286, 0.7263],\n",
      "        [0.5179, 0.3223, 0.7036, 0.8236],\n",
      "        [0.4287, 0.1526, 0.5711, 0.7513],\n",
      "        [0.0937, 0.0475, 0.3261, 0.8516],\n",
      "        [0.3237, 0.2944, 0.4750, 0.8039],\n",
      "        [0.2685, 0.4177, 0.4618, 0.8065],\n",
      "        [0.2787, 0.5649, 0.3752, 0.8006]])\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "model = YOLO('yolov8n.pt')\n",
    "source = r\"C:\\Users\\Rahul\\Desktop\\pexels-jopwell-2422290.jpg\"\n",
    "results = model(source)  # list of Results objects\n",
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bbox outputs\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    print(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([ 0.,  0.,  0.,  0.,  0., 56., 56.])\n",
      "conf: tensor([0.9382, 0.9316, 0.9016, 0.8606, 0.8443, 0.3624, 0.2646])\n",
      "data: tensor([[2.5692e+03, 6.1450e+02, 3.3431e+03, 1.8818e+03, 9.3816e-01, 0.0000e+00],\n",
      "        [1.8646e+03, 8.3511e+02, 2.5331e+03, 2.1338e+03, 9.3156e-01, 0.0000e+00],\n",
      "        [1.5432e+03, 3.9535e+02, 2.0561e+03, 1.9467e+03, 9.0164e-01, 0.0000e+00],\n",
      "        [3.3721e+02, 1.2317e+02, 1.1739e+03, 2.2066e+03, 8.6062e-01, 0.0000e+00],\n",
      "        [1.1653e+03, 7.6282e+02, 1.7101e+03, 2.0830e+03, 8.4432e-01, 0.0000e+00],\n",
      "        [9.6678e+02, 1.0821e+03, 1.6626e+03, 2.0896e+03, 3.6243e-01, 5.6000e+01],\n",
      "        [1.0035e+03, 1.4638e+03, 1.3508e+03, 2.0745e+03, 2.6461e-01, 5.6000e+01]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (2591, 3600)\n",
      "shape: torch.Size([7, 6])\n",
      "xywh: tensor([[2956.1382, 1248.1453,  773.8887, 1267.2889],\n",
      "        [2198.8572, 1484.4658,  668.5414, 1298.7124],\n",
      "        [1799.6504, 1171.0433,  512.8403, 1551.3828],\n",
      "        [ 755.5787, 1164.8755,  836.7355, 2083.4199],\n",
      "        [1437.6892, 1422.8983,  544.7275, 1320.1584],\n",
      "        [1314.6909, 1585.8834,  695.8282, 1007.4973],\n",
      "        [1177.1213, 1769.1218,  347.2915,  610.6758]])\n",
      "xywhn: tensor([[0.8211, 0.4817, 0.2150, 0.4891],\n",
      "        [0.6108, 0.5729, 0.1857, 0.5012],\n",
      "        [0.4999, 0.4520, 0.1425, 0.5988],\n",
      "        [0.2099, 0.4496, 0.2324, 0.8041],\n",
      "        [0.3994, 0.5492, 0.1513, 0.5095],\n",
      "        [0.3652, 0.6121, 0.1933, 0.3888],\n",
      "        [0.3270, 0.6828, 0.0965, 0.2357]])\n",
      "xyxy: tensor([[2569.1938,  614.5009, 3343.0825, 1881.7898],\n",
      "        [1864.5865,  835.1097, 2533.1279, 2133.8220],\n",
      "        [1543.2302,  395.3519, 2056.0706, 1946.7347],\n",
      "        [ 337.2110,  123.1655, 1173.9465, 2206.5854],\n",
      "        [1165.3254,  762.8191, 1710.0530, 2082.9775],\n",
      "        [ 966.7768, 1082.1348, 1662.6050, 2089.6321],\n",
      "        [1003.4756, 1463.7839, 1350.7671, 2074.4597]])\n",
      "xyxyn: tensor([[0.7137, 0.2372, 0.9286, 0.7263],\n",
      "        [0.5179, 0.3223, 0.7036, 0.8236],\n",
      "        [0.4287, 0.1526, 0.5711, 0.7513],\n",
      "        [0.0937, 0.0475, 0.3261, 0.8516],\n",
      "        [0.3237, 0.2944, 0.4750, 0.8039],\n",
      "        [0.2685, 0.4177, 0.4618, 0.8065],\n",
      "        [0.2787, 0.5649, 0.3752, 0.8006]])\n"
     ]
    }
   ],
   "source": [
    "for r in results:\n",
    "    print(r.boxes)  # print the Boxes object containing the detection bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding boxes for 'person' class in result:\n",
      "tensor([[2569.1938,  614.5009, 3343.0825, 1881.7898]])\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([[1864.5865,  835.1097, 2533.1279, 2133.8220]])\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([[1543.2302,  395.3519, 2056.0706, 1946.7347]])\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([[ 337.2110,  123.1655, 1173.9465, 2206.5854]])\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([[1165.3254,  762.8191, 1710.0530, 2082.9775]])\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([], size=(0, 4))\n",
      "Bounding boxes for 'person' class in result:\n",
      "tensor([], size=(0, 4))\n"
     ]
    }
   ],
   "source": [
    "for r in boxes:\n",
    "    # Get the class IDs tensor for this result\n",
    "    class_ids = r.cls\n",
    "\n",
    "    # Get the bounding boxes for this result\n",
    "    bounding_boxes = r.xyxy\n",
    "\n",
    "    # Filter indices where the class ID corresponds to \"person\"\n",
    "    person_indices = (class_ids == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Extract bounding boxes for the \"person\" class\n",
    "    person_boxes = bounding_boxes[person_indices]\n",
    "\n",
    "    # Print or use the bounding boxes for further processing\n",
    "    print(\"Bounding boxes for 'person' class in result:\")\n",
    "    print(person_boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bounding box for 'person' class in result:\n",
      "Bounding box for 'person' class in result:\n",
      "Bounding box for 'person' class in result:\n",
      "Bounding box for 'person' class in result:\n",
      "Bounding box for 'person' class in result:\n"
     ]
    }
   ],
   "source": [
    "for r in boxes:\n",
    "    # Get the class IDs tensor for this result\n",
    "    class_ids = r.cls\n",
    "\n",
    "    # Get the bounding boxes for this result\n",
    "    bounding_boxes = r.xyxy\n",
    "\n",
    "    # Filter indices where the class ID corresponds to \"person\"\n",
    "    person_indices = (class_ids == 0).nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # Extract bounding boxes for the \"person\" class\n",
    "    person_boxes = bounding_boxes[person_indices]\n",
    "\n",
    "    # Check if there are any detections of the \"person\" class with proper tensor\n",
    "    if person_boxes.size(0) > 0:\n",
    "        # Extract the first row of the tensor with proper results\n",
    "        person_box = person_boxes[0]\n",
    "\n",
    "        # Print or use the bounding box for further processing\n",
    "        print(\"Bounding box for 'person' class in result:\")\n",
    "        x1 = int(person_box[0])\n",
    "        y1 = int(person_box[1])\n",
    "        x2 = int(person_box[2])\n",
    "        y2 = int(person_box[3])\n",
    "        image = Image.open(r\"C:\\Users\\Rahul\\Desktop\\pexels-jopwell-2422290.jpg\")\n",
    "        cropped_image = image.crop((x1,  y1,  x2, y2))\n",
    "        cropped_image.show()\n",
    "        \n",
    "        # Further processing of the bounding box\n",
    "        # Your code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
